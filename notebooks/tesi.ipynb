{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-29T09:04:59.960537Z",
     "iopub.status.busy": "2025-03-29T09:04:59.960248Z",
     "iopub.status.idle": "2025-03-29T09:05:20.973203Z",
     "shell.execute_reply": "2025-03-29T09:05:20.972161Z",
     "shell.execute_reply.started": "2025-03-29T09:04:59.960516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from functools import wraps\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:05:20.975034Z",
     "iopub.status.busy": "2025-03-29T09:05:20.974461Z",
     "iopub.status.idle": "2025-03-29T09:05:20.979443Z",
     "shell.execute_reply": "2025-03-29T09:05:20.978597Z",
     "shell.execute_reply.started": "2025-03-29T09:05:20.975008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub.hf_api import HfFolder\n",
    "HfFolder.save_token(\"HF-TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:11:56.180418Z",
     "iopub.status.busy": "2025-03-28T15:11:56.179944Z",
     "iopub.status.idle": "2025-03-28T15:11:56.205566Z",
     "shell.execute_reply": "2025-03-28T15:11:56.204541Z",
     "shell.execute_reply.started": "2025-03-28T15:11:56.180397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:05:24.016593Z",
     "iopub.status.busy": "2025-03-29T09:05:24.016309Z",
     "iopub.status.idle": "2025-03-29T09:05:24.027309Z",
     "shell.execute_reply": "2025-03-29T09:05:24.026453Z",
     "shell.execute_reply.started": "2025-03-29T09:05:24.016572Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Run the following tests:\\nlogger_manager = LoggerManager(\"log.txt\")\\n\\nlogger_manager.write(\"Test message on console.\")  # Write only on console\\n\\nwith LogToFile(logger_manager):\\n    logger_manager.write(\"Test message on file.\")  # Write only on file\\n\\nlogger_manager.write(\"Back to console.\")  # Write only on console\\n\\n# Check file content with\\n!cat log.txt\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create a logger, in kaggle is a mess: https://www.kaggle.com/code/residentmario/notes-on-python-logging/code\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "\n",
    "class LoggerManager:\n",
    "    def __init__(self, file_name):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.logger.propagate = False\n",
    "        self.console_handler = logging.StreamHandler()\n",
    "        self.console_handler.setLevel(logging.INFO)\n",
    "        console_format = logging.Formatter('%(message)s')\n",
    "        self.console_handler.setFormatter(console_format)\n",
    "        if not self.logger.hasHandlers():\n",
    "            self.logger.addHandler(self.console_handler)\n",
    "\n",
    "        self.file_handler = logging.FileHandler(file_name, mode=\"w\", encoding=\"utf-8\")\n",
    "        self.file_handler.setLevel(logging.INFO)\n",
    "        file_format = logging.Formatter('%(message)s')\n",
    "        self.file_handler.setFormatter(file_format)\n",
    "\n",
    "    def write(self, string):\n",
    "        self.logger.info(string)\n",
    "\n",
    "\n",
    "class LogToFile:\n",
    "    \"\"\"Context manager to write temporarly only on file.\"\"\"\n",
    "    def __init__(self, logger_manager):\n",
    "        self.logger_manager = logger_manager\n",
    "        self.logger = logger_manager.logger\n",
    "        self.console_handler = logger_manager.console_handler\n",
    "        self.file_handler = logger_manager.file_handler\n",
    "    \n",
    "    def __enter__(self):\n",
    "        if self.console_handler in self.logger.handlers:\n",
    "            self.logger.removeHandler(self.console_handler)\n",
    "        if self.file_handler not in self.logger.handlers:\n",
    "            self.logger.addHandler(self.file_handler)\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        if self.file_handler in self.logger.handlers:\n",
    "            self.logger.removeHandler(self.file_handler)\n",
    "        if self.console_handler not in self.logger.handlers:\n",
    "            self.logger.addHandler(self.console_handler)\n",
    "\n",
    "\"\"\" Run the following tests:\n",
    "logger_manager = LoggerManager(\"log.txt\")\n",
    "\n",
    "logger_manager.write(\"Test message on console.\")  # Write only on console\n",
    "\n",
    "with LogToFile(logger_manager):\n",
    "    logger_manager.write(\"Test message on file.\")  # Write only on file\n",
    "\n",
    "logger_manager.write(\"Back to console.\")  # Write only on console\n",
    "\n",
    "# Check file content with\n",
    "!cat log.txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:05:26.949991Z",
     "iopub.status.busy": "2025-03-29T09:05:26.949702Z",
     "iopub.status.idle": "2025-03-29T09:05:26.954355Z",
     "shell.execute_reply": "2025-03-29T09:05:26.953391Z",
     "shell.execute_reply.started": "2025-03-29T09:05:26.949970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "logger_manager = LoggerManager(\"file.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:05:27.284488Z",
     "iopub.status.busy": "2025-03-29T09:05:27.284170Z",
     "iopub.status.idle": "2025-03-29T09:05:27.304110Z",
     "shell.execute_reply": "2025-03-29T09:05:27.303156Z",
     "shell.execute_reply.started": "2025-03-29T09:05:27.284461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    examples of models:\n",
    "    microsoft/phi-2\n",
    "    TinyLlama/TinyLlama-1.1B-Chat-v0.6\n",
    "    google/gemma-2-2b-it\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, load_on_init = False):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipe = None\n",
    "        if load_on_init:\n",
    "            self.get_model()\n",
    "            self.get_tokenizer()\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return self.model_name\n",
    "\n",
    "    def format_prompt(self, prompt):\n",
    "        # By default, keep prompt unchanged, some subclasses may have to override this behaviour\n",
    "        # for example deepseek may have to append the <think> tag at the end of the prompt\n",
    "        return prompt\n",
    "        \n",
    "\n",
    "    def get_model(self):\n",
    "        if not self.model:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        return self.model\n",
    "        \n",
    "    def get_tokenizer(self):\n",
    "        if not self.tokenizer:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "        return self.tokenizer\n",
    "\n",
    "    def get_pipeline(self):\n",
    "        if not self.pipe:\n",
    "            self.pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.get_model(),\n",
    "                tokenizer=self.get_tokenizer(),\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.1\n",
    "            )\n",
    "        return self.pipe\n",
    "\n",
    "\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, dataset_fraction = 1, split=\"validation\"):\n",
    "        self.dataset = None\n",
    "        self.dataset_fraction = dataset_fraction\n",
    "        self.split = split\n",
    "     \n",
    "\n",
    "    def get_dataset(self, dataset_name):\n",
    "        if self.dataset:\n",
    "            return self.dataset\n",
    "        if self.split:\n",
    "            self.dataset = load_dataset(dataset_name, split=self.split, trust_remote_code=True)\n",
    "        else:\n",
    "            self.dataset = load_dataset(dataset_name, trust_remote_code=True)\n",
    "        if self.dataset_fraction != None:\n",
    "            num_samples = int(len(self.dataset) * self.dataset_fraction)\n",
    "            self.dataset = self.dataset.shuffle().select(range(num_samples))\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "    def iteration_evaluate_model(self, model, row_idx, row, n_shot, logger_manager = None):\n",
    "        dataset = self.get_dataset()\n",
    "\n",
    "        # Code for n-shot prompting\n",
    "        dataset_keys = list(range(len(dataset)))\n",
    "        dataset_keys_filtered = dataset_keys[:row_idx] + dataset_keys[row_idx + 1:]\n",
    "        dataset_filtered = dataset.select(dataset_keys_filtered)\n",
    "        prompt = \"\"\n",
    "        for i in range(n_shot):\n",
    "            shot_row = random.choice(dataset_filtered)\n",
    "            prompt += self.format_prompt(shot_row) + \" \" + str(self.get_true_answer(shot_row)) + \"\\n\"\n",
    "\n",
    "        # Building the prompt\n",
    "        prompt = prompt + self.format_prompt(row)\n",
    "\n",
    "        # Ask model the prompt\n",
    "        answer = model.get_pipeline()(prompt, return_full_text=False)[0]['generated_text']  \n",
    "\n",
    "\n",
    "        is_llm_answer_correct, is_answer_rejected = self.is_correct(answer, row)\n",
    "\n",
    "        if logger_manager:\n",
    "            logger_manager.write(f\"- Prompt:\\n {prompt}\\n\")\n",
    "            logger_manager.write(f\"- Answer:\\n {answer}\\n\")\n",
    "            logger_manager.write(f\"- True Answer: {self.get_true_answer(row)}\\n\")\n",
    "            logger_manager.write(f\"- Is LLM answer correct? : {is_llm_answer_correct}\\n\")\n",
    "\n",
    "        return is_llm_answer_correct, is_answer_rejected\n",
    "        \n",
    "    \n",
    "    def evaluate_model(self, model, n_shot = 0, logger_manager = None):\n",
    "        correct = 0\n",
    "        rejected = 0\n",
    "        pipe = model.get_pipeline()\n",
    "        dataset = self.get_dataset()\n",
    "        \n",
    "        \n",
    "        with LogToFile(logger_manager):\n",
    "            logger_manager.write(f\"Evaluating {model.get_model_name()} on {self.get_dataset_name()} with {n_shot}-shot\\n\")\n",
    "            \n",
    "        for idx, example in tqdm(enumerate(dataset),total=len(dataset), desc=f\"Evaluating {model.get_model_name()} on {self.get_dataset_name()} with {n_shot}-shot\"):\n",
    "            with LogToFile(logger_manager):\n",
    "                is_correct, is_answer_rejected = self.iteration_evaluate_model(model, idx, example, n_shot, logger_manager)\n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "                if is_answer_rejected:\n",
    "                    rejected += 1\n",
    "                    \n",
    "        accuracy = correct / (len(dataset)) * 100\n",
    "        with LogToFile(logger_manager):\n",
    "            logger_manager.write(f\"\\nFinal accuracy: {accuracy}\")\n",
    "            logger_manager.write(f\"\\nNumber of rejected answers: {rejected}\")\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "\n",
    "class HellaSwag(Dataset):\n",
    "    \"\"\"\n",
    "link: https://huggingface.co/datasets/Rowan/hellaswag\n",
    "Example(cropped)\n",
    "{\n",
    "    \"activity_label\": \"Removing ice from car\",\n",
    "    \"ctx\": \"Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. then\",\n",
    "    \"ctx_a\": \"Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles.\",\n",
    "    \"ctx_b\": \"then\",\n",
    "    \"endings\": \"[\\\", the man adds wax to the windshield and cuts it.\\\", \\\", a person board a ski lift, while two men supporting the head of the per...\",\n",
    "    \"ind\": 4,\n",
    "    \"label\": \"3\",\n",
    "    \"source_id\": \"activitynet~v_-1IBHYS3L-Y\",\n",
    "    \"split\": \"train\",\n",
    "    \"split_type\": \"indomain\"\n",
    "}\n",
    "\n",
    "Note: \n",
    "1. The ctx and the endings may contain tags like [header], [title], [step], [substeps], etc. If we don't remove them, the LLM might mis-interpret the prompt.\n",
    "2. label is from 0-3. We will pose the question to LLM as choose between 4 options indexed 1-4.\n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "    ANSWER = {\n",
    "        0:'A',\n",
    "        1:'B',\n",
    "        2:'C',\n",
    "        3:'D'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, load_on_init = False, dataset_fraction = 1, split = \"validation\"):\n",
    "        super().__init__(dataset_fraction, split)\n",
    "        self.dataset_name = \"hellaswag\"\n",
    "        if load_on_init:\n",
    "            self.get_dataset()\n",
    "        \n",
    "    \n",
    "    def get_dataset(self):\n",
    "        return super().get_dataset(self.dataset_name)\n",
    "\n",
    "    def get_dataset_name(self):\n",
    "        return self.dataset_name\n",
    "\n",
    "    def format_prompt(self,example):\n",
    "        \"\"\"\n",
    "        The example is formatted as\n",
    "\n",
    "        Answer the following multiple choice question. The last line of your response should be in the following format: 'Answer: A/B/C/D' (e.g. 'Answer: A')\n",
    "        Context: Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. then\n",
    "        Which of the following options is the most plausible continuation?\n",
    "        A. The man adds wax to the windshield and cuts it.  \n",
    "        B. A person boards a ski lift, while two men support the head of the person.  \n",
    "        C. The woman walks away and the man starts removing the ice from the car.  \n",
    "        D. The man and woman start dancing on the snowy ground. \n",
    "        \"\"\"\n",
    "        ctx = example['ctx']\n",
    "        ctx = re.sub(r\"\\[.*?\\]\", \"\", ctx).strip() # Remove tags from context\n",
    "        endings = example['endings']\n",
    "        for i in range(len(endings)):\n",
    "            endings[i] = re.sub(r\"\\[.*?\\]\", \"\", endings[i]).strip() # Remove tags from endings\n",
    "        return f\"Answer the following multiple choice question. The last line of your response should be in the following format: 'Answer: A/B/C/D' (e.g. 'Answer: A').\\n Context: {ctx}\\nWhich of the following options is the most plausible continuation?\\nA. {endings[0]}\\nB. {endings[1]}\\nC. {endings[2]}\\nD. {endings[3]}\"\n",
    "\n",
    "    def get_true_answer(self, example):\n",
    "        return f\"Answer: {self.ANSWER[int(example['label'])]}\"\n",
    "\n",
    "    def is_correct(self, model_answer, row): \n",
    "        true_answer = self.ANSWER[int(row['label'])].lower()\n",
    "        prediction = re.search(\"(?i)[\\*\\_]{0,2}Answer[\\*\\_]{0,2}\\s*:[\\s\\*\\_]{0,2}\\s*([A-Z])(?![a-zA-Z0-9])\", model_answer)\n",
    "        \n",
    "        if prediction:\n",
    "            predicition_label = re.search(r\"Answer:(.*)\", prediction.group(0).lower(), re.IGNORECASE).group(1).lower().strip()\n",
    "            return predicition_label == true_answer, False\n",
    "        return False, True\n",
    "\n",
    "\n",
    "class BoolQ(Dataset):\n",
    "    \"\"\"\n",
    "link: https://huggingface.co/datasets/google/boolq\n",
    "Example (cropped):\n",
    "{\n",
    "    \"passage\": \"\\\"All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned...\",\n",
    "    \"question\": \"does ethanol take more energy make that produces\"\n",
    "    \"answer\": false,\n",
    "}\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, load_on_init = False, dataset_fraction = 1, split = \"validation\"):\n",
    "        super().__init__(dataset_fraction, split)\n",
    "        self.dataset_name = \"google/boolq\"\n",
    "        if load_on_init:\n",
    "            self.get_dataset()\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        return super().get_dataset(self.dataset_name)\n",
    "\n",
    "    def get_dataset_name(self):\n",
    "        return self.dataset_name\n",
    "\n",
    "    def format_prompt(self, example):\n",
    "        \"\"\"\n",
    "        Formats the example in this way:\n",
    "        \n",
    "        Passage: All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned...\n",
    "        Question: does ethanol take more energy make that produces\n",
    "        Answer with only True or False:\n",
    "        \"\"\"\n",
    "        # return f\"Passage: {example['passage']}\\nQuestion: {example['question']}\\nAnswer with only True or False:\"\n",
    "        return f\"Answer the following true/false question. The last line of your response should be in the following format: 'Answer: true/false' (e.g. 'Answer: true').\\nPassage: {example['passage']}\\nQuestion: {example['question']}\\n\"\n",
    "\n",
    "\n",
    "    def get_true_answer(self, example):\n",
    "        return f\"Answer: {example['answer']}\"\n",
    "\n",
    "    def is_correct(self, model_answer, row):\n",
    "        \"\"\"\n",
    "        looks for a True of False (ignoring case sensitivity) in the model answer string.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        true_answer = self.get_true_answer(row)\n",
    "        prediction = re.search(r\"(True|False)\", model_answer, re.IGNORECASE)\n",
    "        if prediction: # If there is a true or a false\n",
    "            return str(prediction.group(0).lower()) == str(true_answer).lower() # https://stackoverflow.com/questions/15340582/python-extract-pattern-matches\n",
    "        return False # In every other case, it is not the correct answer :(\n",
    "        \"\"\"\n",
    "\n",
    "        true_answer = self.get_true_answer(row)\n",
    "        prediction = re.search(\"(?i)[\\*\\_]{0,2}Answer[\\*\\_]{0,2}\\s*:[\\s\\*\\_]{0,2}\\s*(true|false)(?![a-zA-Z0-9])\", model_answer)\n",
    "        if prediction:\n",
    "            true_false = re.search(\"(True|False)\", prediction.group(0).lower(), re.IGNORECASE).group(0).lower()\n",
    "            true_true_false = re.search(\"(True|False)\", true_answer, re.IGNORECASE).group(0).lower()\n",
    "            return true_false == true_true_false, False\n",
    "        return False, True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:05:29.171859Z",
     "iopub.status.busy": "2025-03-29T09:05:29.171544Z",
     "iopub.status.idle": "2025-03-29T09:06:12.773393Z",
     "shell.execute_reply": "2025-03-29T09:06:12.772699Z",
     "shell.execute_reply.started": "2025-03-29T09:05:29.171837Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f101c3fb7c1b423c96fc83ac6dc61d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a3546708334583a1888f9e35cf7a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab744e41b23476e8145b45aedee3c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6179cc2265604849b5c31b009c8506ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fb6906d930467f91a68a087b728080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884d6677aee04377a31690c32621cadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42064e86fc45470aa630d74ffbe8147b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da8003c990e468e895bd62c02d98767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0619f3520b4d7c8bccfe23ad4d1388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd9c38801b4469c8fc9f2b60fdc6cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75e364164f14841bfd23b147553db7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_2b = Model(\"google/gemma-2-2b-it\", load_on_init = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:06:19.150885Z",
     "iopub.status.busy": "2025-03-29T09:06:19.150563Z",
     "iopub.status.idle": "2025-03-29T09:06:28.511744Z",
     "shell.execute_reply": "2025-03-29T09:06:28.510850Z",
     "shell.execute_reply.started": "2025-03-29T09:06:19.150863Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aba2759d0194b99b588ca1fd5af94e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2487ac56de42497982d36e4ec70bec8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hellaswag.py:   0%|          | 0.00/4.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83e729e7c124155a64169296916c9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/2.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0799b51bb5cb49fd8e8f00c11bec5d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/47.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0d7b5b3e9b464ab29c4917f7e0e5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1c412a587f4e778dbf1bd7782e0c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45f38942f1045e38a6e6c452284c13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/39905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b708e97f2247ffa61d8f9da57fd51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9038ee2ddfb64af89f46fe98ca22112a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hellaswag = HellaSwag(load_on_init=True, dataset_fraction = 1, split = \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:06:28.513141Z",
     "iopub.status.busy": "2025-03-29T09:06:28.512900Z",
     "iopub.status.idle": "2025-03-29T09:06:31.273127Z",
     "shell.execute_reply": "2025-03-29T09:06:31.272461Z",
     "shell.execute_reply.started": "2025-03-29T09:06:28.513119Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n",
      "- Prompt:\n",
      " Answer the following multiple choice question. The last line of your response should be in the following format: 'Answer: A/B/C/D' (e.g. 'Answer: A').\n",
      " Context: How to avoid mercury in your skin products  Recognize common products that use mercury.  There may be mercury lurking in many of your skin care products. However, some are more likely to contain it.\n",
      "Which of the following options is the most plausible continuation?\n",
      "A. The common types include :  Mineral oil prescription hydrogen peroxide baking soda diogenes (example used are acidophilus and ciprofloxacin) dissolved retinoids  Rinse your face.  Use a mild, baby soap or bathwater.\n",
      "B. Check any of the following to see if they contain mercury :  Skin creams, especially anti-aging and skin lightening beauty and antiseptic soaps lotions  Read the product label for mercury synonyms.  Since mercury doesn't have a distinctive smell or color, the only way to tell if it is in a product is to read the labeling.\n",
      "C. Common products used in underarm hygiene include :  Moisturizers acne-effective fabric. These products are generally available for cosmetic use, including groceries, jeans, and sweatshirts.\n",
      "D. Regardless of the product you choose, when you apply the product, you should wait 24 hours before applying it to your face or hair.  While \" pure pure \" generally refers to most cosmetics, make sure to refer to the label to see if it is over the counter, and limit your use to only half and half.\n",
      "\n",
      "- Answer:\n",
      " \n",
      "\n",
      "\n",
      "Answer: B \n",
      "\n",
      "\n",
      "- True Answer: Answer: B\n",
      "\n",
      "- Is LLM answer correct? : True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_idx, row = random.choice(list(enumerate(hellaswag.get_dataset())))\n",
    "n_shot = 0\n",
    "hellaswag.iteration_evaluate_model(gemma_2b, row_idx, row, n_shot, logger_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:06:59.600347Z",
     "iopub.status.busy": "2025-03-29T09:06:59.600030Z",
     "iopub.status.idle": "2025-03-29T09:07:13.691472Z",
     "shell.execute_reply": "2025-03-29T09:07:13.690632Z",
     "shell.execute_reply.started": "2025-03-29T09:06:59.600325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Prompt:\n",
      " Answer the following multiple choice question. The last line of your response should be in the following format: 'Answer: A/B/C/D' (e.g. 'Answer: A').\n",
      " Context: How to take care of ladybugs  Purchase ladybugs.  If you have a garden or a greenhouse, you should consider adding more ladybugs to the environment. Ladybugs are very useful for controlling aphids.\n",
      "Which of the following options is the most plausible continuation?\n",
      "A. Aphids are tiny insects that can destroy vegetable and flower gardens.  Ladybugs tend to hibernate during winter months.\n",
      "B. Ladybugs can be sold as small boxes, pots, or bowls.  Ladybugs come in all shapes and sizes.\n",
      "C. You might try to keep ladybugs away from the garden. You can also buy ladybugs from a local store that sells insect products.\n",
      "D. Ladybugs can be especially helpful to keep livestock and other animals away as well as washing your vegetable and insect products.  Purchase the ladybugs' natural habitats. Answer: A\n",
      "Answer the following multiple choice question. The last line of your response should be in the following format: 'Answer: A/B/C/D' (e.g. 'Answer: A').\n",
      " Context: How to avoid mercury in your skin products  Recognize common products that use mercury.  There may be mercury lurking in many of your skin care products. However, some are more likely to contain it.\n",
      "Which of the following options is the most plausible continuation?\n",
      "A. The common types include :  Mineral oil prescription hydrogen peroxide baking soda diogenes (example used are acidophilus and ciprofloxacin) dissolved retinoids  Rinse your face.  Use a mild, baby soap or bathwater.\n",
      "B. Check any of the following to see if they contain mercury :  Skin creams, especially anti-aging and skin lightening beauty and antiseptic soaps lotions  Read the product label for mercury synonyms.  Since mercury doesn't have a distinctive smell or color, the only way to tell if it is in a product is to read the labeling.\n",
      "C. Common products used in underarm hygiene include :  Moisturizers acne-effective fabric. These products are generally available for cosmetic use, including groceries, jeans, and sweatshirts.\n",
      "D. Regardless of the product you choose, when you apply the product, you should wait 24 hours before applying it to your face or hair.  While \" pure pure \" generally refers to most cosmetics, make sure to refer to the label to see if it is over the counter, and limit your use to only half and half.\n",
      "\n",
      "- Answer:\n",
      "   Answer: B\n",
      "Answer the following multiple choice question. The last line of your response should be in the following format: 'Answer: A/B/C/D' (e.g. 'Answer: A').\n",
      " Context: How to take care of a sick cat\n",
      "If your cat is sick, it is important to take care of them.  You should take your cat to the vet.  If your cat is sick, it is important to take care of them.  You should take your cat to the vet.\n",
      "Which of the following options is the most plausible continuation?\n",
      "A.  You should make sure your cat has plenty of fresh water and food.  You should also make sure your cat has a comfortable place to rest.\n",
      "B.  You should make sure your cat has plenty of fresh water and food.  You should also make sure your cat has a comfortable place to rest.  You should also make sure your cat has a comfortable place to rest.\n",
      "C.  You should make sure your cat has plenty of fresh water and food.  You should also make sure your cat has a comfortable place to rest.  You should also make sure your cat has a comfortable place to rest.  You should also make sure\n",
      "\n",
      "- True Answer: Answer: B\n",
      "\n",
      "- Is LLM answer correct? : True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row_idx, row = random.choice(list(enumerate(boolq.get_dataset())))\n",
    "n_shot = 1\n",
    "hellaswag.iteration_evaluate_model(gemma_2b, row_idx, row, n_shot, logger_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hellaswag.evaluate_model(gemma_2b, 1, logger_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!tail -n 3 file.log"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
