{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom datasets import load_dataset\nimport torch\nfrom tqdm import tqdm\nimport re\nfrom functools import wraps\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:50:37.077287Z","iopub.execute_input":"2025-02-14T16:50:37.077534Z","iopub.status.idle":"2025-02-14T16:50:58.727753Z","shell.execute_reply.started":"2025-02-14T16:50:37.077511Z","shell.execute_reply":"2025-02-14T16:50:58.726836Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\"\"\"\n    Create a logger, in kaggle is a mess: https://www.kaggle.com/code/residentmario/notes-on-python-logging/code\n\"\"\"\n\nimport logging\n\nclass LoggerManager:\n    def __init__(self, file_name):\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.INFO)\n        self.logger.propagate = False\n        self.console_handler = logging.StreamHandler()\n        self.console_handler.setLevel(logging.INFO)\n        console_format = logging.Formatter('%(message)s')\n        self.console_handler.setFormatter(console_format)\n        if not self.logger.hasHandlers():\n            self.logger.addHandler(self.console_handler)\n\n        self.file_handler = logging.FileHandler(file_name, mode=\"w\", encoding=\"utf-8\")\n        self.file_handler.setLevel(logging.INFO)\n        file_format = logging.Formatter('%(message)s')\n        self.file_handler.setFormatter(file_format)\n\n    def write(self, string):\n        self.logger.info(string)\n\n\nclass LogToFile:\n    \"\"\"Context manager to write temporarly only on file.\"\"\"\n    def __init__(self, logger_manager):\n        self.logger_manager = logger_manager\n        self.logger = logger_manager.logger\n        self.console_handler = logger_manager.console_handler\n        self.file_handler = logger_manager.file_handler\n    \n    def __enter__(self):\n        if self.console_handler in self.logger.handlers:\n            self.logger.removeHandler(self.console_handler)\n        if self.file_handler not in self.logger.handlers:\n            self.logger.addHandler(self.file_handler)\n    \n    def __exit__(self, exc_type, exc_value, traceback):\n        if self.file_handler in self.logger.handlers:\n            self.logger.removeHandler(self.file_handler)\n        if self.console_handler not in self.logger.handlers:\n            self.logger.addHandler(self.console_handler)\n\n\"\"\" Run the following tests:\nlogger_manager = LoggerManager(\"log.txt\")\n\nlogger_manager.write(\"Test message on console.\")  # Write only on console\n\nwith LogToFile(logger_manager):\n    logger_manager.write(\"Test message on file.\")  # Write only on file\n\nlogger_manager.write(\"Back to console.\")  # Write only on console\n\n# Check file content with\n!cat log.txt\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:51:22.286233Z","iopub.execute_input":"2025-02-14T16:51:22.286585Z","iopub.status.idle":"2025-02-14T16:51:22.297407Z","shell.execute_reply.started":"2025-02-14T16:51:22.286558Z","shell.execute_reply":"2025-02-14T16:51:22.296546Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"' Run the following tests:\\nlogger_manager = LoggerManager(\"log.txt\")\\n\\nlogger_manager.write(\"Test message on console.\")  # Write only on console\\n\\nwith LogToFile(logger_manager):\\n    logger_manager.write(\"Test message on file.\")  # Write only on file\\n\\nlogger_manager.write(\"Back to console.\")  # Write only on console\\n\\n# Check file content with\\n!cat log.txt\\n'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"logger_manager = LoggerManager(\"file.log\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:51:25.327495Z","iopub.execute_input":"2025-02-14T16:51:25.327814Z","iopub.status.idle":"2025-02-14T16:51:25.332640Z","shell.execute_reply.started":"2025-02-14T16:51:25.327788Z","shell.execute_reply":"2025-02-14T16:51:25.331459Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class Model:\n    def __init__(self, model_name, load_on_init = False):\n        self.model_name = model_name\n        self.model = None\n        self.tokenizer = None\n        self.pipe = None\n        if load_on_init:\n            self.get_model()\n            self.get_tokenizer()\n\n    def get_model_name(self):\n        return self.model_name\n\n    def format_prompt(self, prompt):\n        # By default, keep prompt unchanged, some subclasses may have to override this behaviour\n        # for example deepseek may have to append the <think> tag at the end of the prompt\n        return prompt\n        \n\n    def get_model(self):\n        if not self.model:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.model_name,\n                torch_dtype=torch.float16,\n                device_map=\"auto\",\n                trust_remote_code=True\n            )\n        return self.model\n        \n    def get_tokenizer(self):\n        if not self.tokenizer:\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n        return self.tokenizer\n\n    def get_pipeline(self):\n        if not self.pipe:\n            self.pipe = pipeline(\n                \"text-generation\",\n                model=self.get_model(),\n                tokenizer=self.get_tokenizer(),\n                max_new_tokens=256,\n                temperature=0.1\n            )\n        return self.pipe\n\nclass Phi2(Model):\n    def __init__(self, load_on_init=False):\n        super().__init__(\"microsoft/phi-2\", load_on_init)\n\n    def clean_answer(self, answer, prompt):\n        return answer\n\nclass TinyLLamaSmall(Model):\n    def __init__(self, load_on_init=False):\n        super().__init__(\"TinyLlama/TinyLlama-1.1B-Chat-v0.6\", load_on_init)\n        \n    def clean_answer(self, answer, prompt):\n        return answer\n\nclass DeepSeekSmall(Model):\n    def __init__(self, load_on_init=False):\n        super().__init__(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", load_on_init)\n\n    def format_prompt(self, prompt):\n        # Add a <think> tag as suggested. https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n        return prompt + \"<think>\\n\"\n\n    def clean_answer(self, answer, prompt):\n        return answer\n\nclass Dataset():\n    def __init__(self, dataset_fraction = 1, split=\"validation\"):\n        self.dataset = None\n        self.dataset_fraction = dataset_fraction\n        self.split = split\n     \n\n    def get_dataset(self, dataset_name):\n        if self.dataset:\n            return self.dataset\n        if self.split:\n            self.dataset = load_dataset(dataset_name, split=self.split)\n        else:\n            self.dataset = load_dataset(dataset_name)\n        if self.dataset_fraction != None:\n            num_samples = int(len(self.dataset) * self.dataset_fraction)\n            self.dataset = self.dataset.shuffle().select(range(num_samples))\n        return self.dataset\n\n\n    def iteration_evaluate_model(self, model, row_idx, row, n_shot, logger_manager = None):\n        dataset = self.get_dataset()\n\n        dataset_keys = list(range(len(dataset)))\n        dataset_keys_filtered = dataset_keys[:row_idx] + dataset_keys[row_idx + 1:]\n        dataset_filtered = dataset.select(dataset_keys_filtered)\n        prompt = \"\"\n        for i in range(n_shot):\n            shot_row = random.choice(dataset_filtered)\n            prompt += self.format_prompt(shot_row) + \" \" + str(self.get_true_answer(shot_row)) + \"\\n\"\n        \n        prompt = prompt + self.format_prompt(row)\n        # prompt = model.format_prompt(prompt)\n\n        answer = model.get_pipeline()(prompt, return_full_text=False)[0]['generated_text']  \n\n        cleaned_answer = model.clean_answer(answer, prompt) # Each model has a unique way to reply\n        cleaned_answer = self.clean_answer(cleaned_answer, prompt) # Each dataset has a unique way to clean the answer\n        true_answer = self.get_true_answer(row)\n\n\n        is_llm_answer_correct = self.is_correct(cleaned_answer, true_answer)\n\n        if logger_manager:\n            logger_manager.write(f\"- Prompt:\\n {prompt}\\n\")\n            logger_manager.write(f\"- Answer:\\n {answer}\\n\")\n            logger_manager.write(f\"- Cleaned Answer:\\n {cleaned_answer}\\n\")\n            logger_manager.write(f\"- True Answer: {true_answer}\\n\")\n            logger_manager.write(f\"- Is LLM answer correct? : {is_llm_answer_correct}\\n\")\n\n        return is_llm_answer_correct\n        \n    \n    def evaluate_model(self, model, n_shot = 0, logger_manager = None):\n        correct = 0\n        pipe = model.get_pipeline()\n        dataset = self.get_dataset()\n        \n        with LogToFile(logger_manager):\n            logger_manager.write(f\"Evaluating {model.get_model_name()} on {self.get_dataset_name()}\\n\")\n            \n        for idx, example in tqdm(enumerate(dataset),total=len(dataset), desc=f\"Evaluating {model.get_model_name()} on {self.get_dataset_name()}\"):\n            with LogToFile(logger_manager):\n                is_correct = self.iteration_evaluate_model(model, idx, example, n_shot, logger_manager)\n                if is_correct:\n                    correct += 1\n        accuracy = correct / (len(dataset)) * 100\n        with LogToFile(logger_manager):\n            logger_manager.write(f\"\\nFinal accuracy: {accuracy}\")\n        return accuracy\n\n\n\nclass HellaSwag(Dataset):\n    \"\"\"\nlink: https://huggingface.co/datasets/Rowan/hellaswag\nExample(cropped)\n{\n    \"activity_label\": \"Removing ice from car\",\n    \"ctx\": \"Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. then\",\n    \"ctx_a\": \"Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles.\",\n    \"ctx_b\": \"then\",\n    \"endings\": \"[\\\", the man adds wax to the windshield and cuts it.\\\", \\\", a person board a ski lift, while two men supporting the head of the per...\",\n    \"ind\": 4,\n    \"label\": \"3\",\n    \"source_id\": \"activitynet~v_-1IBHYS3L-Y\",\n    \"split\": \"train\",\n    \"split_type\": \"indomain\"\n}\n\nNote: \n1. The ctx and the endings may contain tags like [header], [title], [step], [substeps], etc. If we don't remove them, the LLM might mis-interpret the prompt.\n2. label is from 0-3. We will pose the question to LLM as choose between 4 options indexed 1-4.\n\n    \"\"\"  \n    \n    def __init__(self, load_on_init = False, dataset_fraction = 1, split = \"validation\"):\n        super().__init__(dataset_fraction, split)\n        self.dataset_name = \"hellaswag\"\n        if load_on_init:\n            self.get_dataset()\n        \n    \n    def get_dataset(self):\n        return super().get_dataset(self.dataset_name)\n\n    def get_dataset_name(self):\n        return self.dataset_name\n\n    def format_prompt(self,example):\n        \"\"\"\n        The example is formatted as\n        \n        Context: Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. then\n        Which of the following options is the most plausible continuation?\n        1. The man adds wax to the windshield and cuts it.  \n        2. A person boards a ski lift, while two men support the head of the person.  \n        3. The woman walks away and the man starts removing the ice from the car.  \n        4. The man and woman start dancing on the snowy ground. \n        Respond with only the number of the most plausible option: \n        \"\"\"\n        ctx = example['ctx']\n        ctx = re.sub(r\"\\[.*?\\]\", \"\", ctx).strip() # Remove tags from context\n        endings = example['endings']\n        for i in range(len(endings)):\n            endings[i] = re.sub(r\"\\[.*?\\]\", \"\", endings[i]).strip() # Remove tags from endings\n        return f\"Context: {ctx}\\nWhich of the following options is the most plausible continuation?\\n1. {endings[0]}\\n2. {endings[1]}\\n3. {endings[2]}\\n4. {endings[3]}\\nRespond with only the number of the most plausible option:\"\n\n    def clean_answer(self, answer, prompt):\n        return answer.split(\"\\n\")[0] # Take only first row of response, the other rows are usually the explaining\n\n    def get_true_answer(self, example):\n        return str(int(example[\"label\"]) + 1)\n\n    def is_correct(self, model_answer, true_answer): \n        \"\"\"\n        true_answer is already 1-indexed by get_true_answer. \n        true_answer \\in [1,2,3,4]\n        \"\"\"\n        return str(true_answer) in model_answer\n\n\nclass SquadV2(Dataset):\n    \"\"\"\nlink: https://huggingface.co/datasets/rajpurkar/squad_v2\nExample(cropped)\n{\n    \"answers\": {\n        \"answer_start\": [94, 87, 94, 94],\n        \"text\": [\"10th and 11th centuries\", \"in the 10th and 11th centuries\", \"10th and 11th centuries\", \"10th and 11th centuries\"]\n    },\n    \"context\": \"\\\"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave thei...\",\n    \"question\": \"When were the Normans in Normandy?\",\n    \"title\": \"Normans\"\n}\n\nNote: there can be no correct answer, i.e \nExample\n{\n    \"context\": \"\\\"The \"West Side\" of Fresno, also often called \"Southwest Fresno\", is one of the oldest neighborhoods in the city. The neighborhood lies southwest of the 99 freeway (which divides it from Downtown Fresno), west of the 41 freeway and south of Nielsen Ave (or the newly constructed 180 Freeway), and extends to the city limits to the west and south. The neighborhood is traditionally considered to be the center of Fresno's African-American community. It is culturally diverse and also includes significant Mexican-American and Asian-American (principally Hmong or Laotian) populations.\n    \"question\": \"What is significant about the age of Downtown Fresno?\"\n    \"answers\": {\n        \"answer_start\": []\n        \"text\": []\n    }\n}\n    \"\"\"\n    \n    def __init__(self, load_on_init = False, dataset_fraction = 1, split = \"validation\"):\n        super().__init__(dataset_fraction, split)\n        self.dataset_name = \"rajpurkar/squad_v2\"\n        if load_on_init:\n            self.get_dataset()\n    \n    def get_dataset(self):\n        return super().get_dataset(self.dataset_name)\n\n    def get_dataset_name(self):\n        return self.dataset_name\n\n    def format_prompt(self, example):\n        return f\"Context: {example['context']} Question: {example['question']} Answer:\"\n\n    def clean_answer(self, answer, prompt):\n        if answer.startswith(prompt):\n            answer = answer[len(prompt):].strip()\n        return answer\n\n    def get_true_answer(self, example):\n        return example[\"answers\"][\"text\"]\n\n    def is_correct(self, model_answer, true_answer):\n        llm_no_answer = \"No response\" in model_answer or model_answer == \"\"\n        no_answer = len(true_answer) == 0\n        return (no_answer and llm_no_answer) or (model_answer in true_answer)\n\n\n\nclass BoolQ(Dataset):\n    \"\"\"\nlink: https://huggingface.co/datasets/google/boolq\nExample (cropped):\n{\n    \"passage\": \"\\\"All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned...\",\n    \"question\": \"does ethanol take more energy make that produces\"\n    \"answer\": false,\n}\n    \n    \"\"\"\n    def __init__(self, load_on_init = False, dataset_fraction = 1, split = \"validation\"):\n        super().__init__(dataset_fraction, split)\n        self.dataset_name = \"google/boolq\"\n        if load_on_init:\n            self.get_dataset()\n    \n    def get_dataset(self):\n        return super().get_dataset(self.dataset_name)\n\n    def get_dataset_name(self):\n        return self.dataset_name\n\n    def format_prompt(self, example):\n        \"\"\"\n        Formats the example in this way:\n        \n        Passage: All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned...\n        Question: does ethanol take more energy make that produces\n        Answer with only True or False:\n        \"\"\"\n        return f\"Passage: {example['passage']}\\nQuestion: {example['question']}\\nAnswer with only True or False:\"\n\n    def clean_answer(self, answer, prompt):\n        return answer.split(\"\\n\")[0] # Take only first row of response, the other rows are usually the explaining\n\n    def get_true_answer(self, example):\n        return example[\"answer\"]\n\n    def is_correct(self, model_answer, true_answer):\n        \"\"\"\n        looks for a True of False (ignoring case sensitivity) in the model answer string.\n        \"\"\"\n        prediction = re.search(r\"(True|False)\", model_answer, re.IGNORECASE)\n        if prediction: # If there is a true or a false\n            return str(prediction.group(0).lower()) == str(true_answer).lower() # https://stackoverflow.com/questions/15340582/python-extract-pattern-matches\n        return False # In every other case, it is not the correct answer :(\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:51:27.234096Z","iopub.execute_input":"2025-02-14T16:51:27.234376Z","iopub.status.idle":"2025-02-14T16:51:27.257587Z","shell.execute_reply.started":"2025-02-14T16:51:27.234356Z","shell.execute_reply":"2025-02-14T16:51:27.256505Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Experiments on HellaSwag dataset","metadata":{}},{"cell_type":"code","source":"tinyLLamaSmall = TinyLLamaSmall(load_on_init=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:51:35.988336Z","iopub.execute_input":"2025-02-14T16:51:35.988675Z","iopub.status.idle":"2025-02-14T16:52:32.819527Z","shell.execute_reply.started":"2025-02-14T16:51:35.988651Z","shell.execute_reply":"2025-02-14T16:52:32.818811Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/699 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5f5922a3a5044cd90bed193945fe8c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f7d13c295cf423b8602541bb0bec917"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78081af60f1a44058a8d34b0bce4a306"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f69bdf78ec6b472e85dd648647df6470"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d206346c087343568f445852c1c81d91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"818fbee9feba4c63b172261bf2cb41b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e25121cd06874df89b5fc3131852abb9"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"hellaSwag = HellaSwag(load_on_init=True, dataset_fraction = 0.005, split = \"validation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:55:18.491705Z","iopub.execute_input":"2025-02-14T14:55:18.492008Z","iopub.status.idle":"2025-02-14T14:55:31.231594Z","shell.execute_reply.started":"2025-02-14T14:55:18.491985Z","shell.execute_reply":"2025-02-14T14:55:31.230941Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bfec2a7952544d887581a8cab006005"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"hellaswag.py:   0%|          | 0.00/4.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c03b166078e41649902c60dced964d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json:   0%|          | 0.00/2.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65478a0f564544789f3e35b6a14cbf5a"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/47.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e75ba835b0e44ecbd9eab18280ee08f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7809487e3ac94aaaa2d5beb698953fc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/12.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dbe5cdf96e54f8c8d661a8e0cc71711"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/39905 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fd678be5bac48cb8a8e5b6aef088070"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"663de69849d746eda193072d17949c8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10042 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"694fe2a85b844609addec7d63a363895"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"row_idx, row = random.choice(list(enumerate(hellaSwag.get_dataset())))\nn_shot = 0\nhellaSwag.iteration_evaluate_model(tinyLLamaSmall, row_idx, row, n_shot, logger_manager)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:31:36.126386Z","iopub.execute_input":"2025-02-14T13:31:36.126694Z","iopub.status.idle":"2025-02-14T13:31:37.290722Z","shell.execute_reply.started":"2025-02-14T13:31:36.126643Z","shell.execute_reply":"2025-02-14T13:31:37.290010Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n- Prompt:\n Context: How to wear a trench coat  Choose a long trench coat if you're tall.  Trench coats come in several different lengths, but this is a common length. Long trench coats can come down past the knee or even just above the ankle.\nWhich of the following options is the most plausible continuation?\n1. Long trench coats are ideal for taller people, but they may make short people appear even shorter.  Wear shoes with a heel if you're a shorter person wearing a long trench coat.\n2. Select a coat that reaches just below the knees to help accentuate your figure. If you're tall, look for trench coats that reach the shins, or underarms.\n3. Trench coats are long for coats made by men so you want to choose one with a solid material.  Trench coats come in different lengths, such as a waist length trench coat, a neck length trench coat, or a shoulder length trench coat.\n4. Several styles of trench coats have ties that can be worn flat to look outfit-appropriate, or in a wide variety of styles, depending on which pant line you choose.  Men's trench coats are usually held together with ties that are assigned to men.\nRespond with only the number of the most plausible option:\n\n- Answer:\n  2.\nCan you provide a summary of the options for selecting a trench coat for tall people?\n\n- Cleaned Answer:\n  2.\n\n- True Answer: 1\n\n- Is LLM answer correct? : False\n\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"hellaSwag.evaluate_model(tinyLLamaSmall, n_shot=0, logger_manager)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:31:37.291537Z","iopub.execute_input":"2025-02-14T13:31:37.291793Z","iopub.status.idle":"2025-02-14T13:33:46.495245Z","shell.execute_reply.started":"2025-02-14T13:31:37.291770Z","shell.execute_reply":"2025-02-14T13:33:46.494549Z"}},"outputs":[{"name":"stderr","text":"Evaluating TinyLlama/TinyLlama-1.1B-Chat-v0.6 on hellaswag:  18%|█▊        | 9/50 [00:16<01:13,  1.78s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nEvaluating TinyLlama/TinyLlama-1.1B-Chat-v0.6 on hellaswag: 100%|██████████| 50/50 [02:09<00:00,  2.58s/it]\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"16.0"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"!tail -n 1 file.log","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:36:08.319312Z","iopub.execute_input":"2025-02-14T13:36:08.319649Z","iopub.status.idle":"2025-02-14T13:36:08.495877Z","shell.execute_reply.started":"2025-02-14T13:36:08.319619Z","shell.execute_reply":"2025-02-14T13:36:08.494834Z"}},"outputs":[{"name":"stdout","text":"16.0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"16 percent accuracy using 0.001 dataset fraction","metadata":{}},{"cell_type":"code","source":"phi2 = Phi2(load_on_init=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:30:47.625686Z","iopub.execute_input":"2025-02-14T14:30:47.625992Z","iopub.status.idle":"2025-02-14T14:33:04.378290Z","shell.execute_reply.started":"2025-02-14T14:30:47.625968Z","shell.execute_reply":"2025-02-14T14:33:04.377581Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb93c11831d64f668dda550f5c649b24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"496c83c969d84c28bff755b2584dd099"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b62495e58bfe4e2092bef59952781b12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfd588b973714f0db2dfbb3531a8ca4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e521c8a6d1456f8cc808002f64fbbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd2efc6c0cb54fefbbb231285dd49c29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10b255b7b57c4f4e85ac62a2fa5ad5a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11a5c3247e114d58acc5c849910458c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eb7d03b7a3a4bc8a1936cd966d04330"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ec8c30ba08646bea48fa8522c2647f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ccb1d2c35644ee995e29399cc044eaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdc633eb36fb42cdaa497fdace819912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6165ea656f8c4fb7bc61a05c0778b656"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"row_idx, row = random.choice(list(enumerate(hellaSwag.get_dataset())))\nn_shot = 0\nhellaSwag.iteration_evaluate_model(phi2, row_idx, row, n_shot, logger_manager)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:47:27.888027Z","iopub.execute_input":"2025-02-14T13:47:27.888363Z","iopub.status.idle":"2025-02-14T13:47:35.069385Z","shell.execute_reply.started":"2025-02-14T13:47:27.888335Z","shell.execute_reply":"2025-02-14T13:47:35.068530Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n- Prompt:\n Context: A man is seen sitting in a tube speaking and leads into people walking around carrying tubes. shots of the water\nWhich of the following options is the most plausible continuation?\n1. had the people walking along with water crashing into their tubes.\n2. ride in a tube are shown around the tube.\n3. is shown followed by people riding down a river on the tubes.\n4. and people occasionally riding the tubes and getting ready to jump all around the world.\nRespond with only the number of the most plausible option:\n\n- Answer:\n \n1. 2\n2. 3\n3. 4\n4. 1\nAnswer:\n1. 2\nExplanation:\nThe most plausible continuation is option 2, ride in a tube are shown around the tube.\n\nExercise 2:\nContext: A man is seen sitting in a tube speaking and leads into people walking around carrying tubes. shots of the water\nWhich of the following options is the most plausible continuation?\n1. had the people walking along with water crashing into their tubes.\n2. ride in a tube are shown around the tube.\n3. is shown followed by people riding down a river on the tubes.\n4. and people occasionally riding the tubes and getting ready to jump all around the world.\nRespond with only the number of the most plausible option:\n1. 2\n2. 3\n3. 4\n4. 1\nAnswer:\n2. 3\nExplanation:\nThe most plausible continuation is option 3, is shown followed by people riding down a river on the tubes.\n\nExercise 3:\nContext: A man is seen sitting in a tube speaking and leads into people walking around carrying tubes. shots of the water\nWhich of the following options is the most\n\n- Cleaned Answer:\n \n\n- True Answer: 3\n\n- Is LLM answer correct? : False\n\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"hellaSwag.evaluate_model(phi2, n_shot=0, logger_manager)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:41:29.557346Z","iopub.execute_input":"2025-02-14T13:41:29.557686Z","iopub.status.idle":"2025-02-14T13:46:59.711306Z","shell.execute_reply.started":"2025-02-14T13:41:29.557638Z","shell.execute_reply":"2025-02-14T13:46:59.710433Z"}},"outputs":[{"name":"stderr","text":"Evaluating microsoft/phi-2 on hellaswag:   0%|          | 0/50 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:   2%|▏         | 1/50 [00:07<06:06,  7.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:   4%|▍         | 2/50 [00:14<05:54,  7.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:   6%|▌         | 3/50 [00:22<05:46,  7.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:   8%|▊         | 4/50 [00:29<05:42,  7.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  10%|█         | 5/50 [00:34<04:57,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  12%|█▏        | 6/50 [00:37<03:48,  5.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  14%|█▍        | 7/50 [00:42<03:48,  5.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  16%|█▌        | 8/50 [00:50<04:14,  6.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  18%|█▊        | 9/50 [00:58<04:29,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  20%|██        | 10/50 [01:05<04:32,  6.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  22%|██▏       | 11/50 [01:12<04:31,  6.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  24%|██▍       | 12/50 [01:20<04:31,  7.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  26%|██▌       | 13/50 [01:24<03:51,  6.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  28%|██▊       | 14/50 [01:32<03:59,  6.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  30%|███       | 15/50 [01:34<03:07,  5.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  32%|███▏      | 16/50 [01:41<03:13,  5.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  34%|███▍      | 17/50 [01:48<03:27,  6.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  36%|███▌      | 18/50 [01:56<03:34,  6.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  38%|███▊      | 19/50 [02:03<03:27,  6.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  40%|████      | 20/50 [02:09<03:15,  6.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  42%|████▏     | 21/50 [02:16<03:18,  6.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  44%|████▍     | 22/50 [02:20<02:43,  5.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  46%|████▌     | 23/50 [02:27<02:51,  6.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  48%|████▊     | 24/50 [02:35<02:54,  6.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  50%|█████     | 25/50 [02:43<02:54,  6.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  52%|█████▏    | 26/50 [02:50<02:52,  7.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  54%|█████▍    | 27/50 [02:54<02:19,  6.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  56%|█████▌    | 28/50 [03:00<02:13,  6.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  58%|█████▊    | 29/50 [03:07<02:15,  6.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  60%|██████    | 30/50 [03:15<02:15,  6.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  62%|██████▏   | 31/50 [03:22<02:11,  6.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  64%|██████▍   | 32/50 [03:29<02:07,  7.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  66%|██████▌   | 33/50 [03:37<02:02,  7.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  68%|██████▊   | 34/50 [03:37<01:23,  5.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  70%|███████   | 35/50 [03:45<01:28,  5.92s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  72%|███████▏  | 36/50 [03:52<01:28,  6.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  74%|███████▍  | 37/50 [04:00<01:26,  6.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  76%|███████▌  | 38/50 [04:07<01:20,  6.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  78%|███████▊  | 39/50 [04:14<01:15,  6.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  80%|████████  | 40/50 [04:21<01:10,  7.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  82%|████████▏ | 41/50 [04:29<01:04,  7.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  84%|████████▍ | 42/50 [04:36<00:58,  7.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  86%|████████▌ | 43/50 [04:44<00:51,  7.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  88%|████████▊ | 44/50 [04:51<00:43,  7.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  90%|█████████ | 45/50 [04:53<00:28,  5.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  92%|█████████▏| 46/50 [05:00<00:24,  6.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  94%|█████████▍| 47/50 [05:08<00:19,  6.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  96%|█████████▌| 48/50 [05:15<00:13,  6.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag:  98%|█████████▊| 49/50 [05:22<00:06,  6.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nEvaluating microsoft/phi-2 on hellaswag: 100%|██████████| 50/50 [05:30<00:00,  6.60s/it]\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"4.0"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"# Experiment on BoolQ dataset","metadata":{}},{"cell_type":"code","source":"boolq = BoolQ(load_on_init=True, dataset_fraction = 0.005, split = \"validation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:52:32.820736Z","iopub.execute_input":"2025-02-14T16:52:32.821051Z","iopub.status.idle":"2025-02-14T16:52:36.464070Z","shell.execute_reply.started":"2025-02-14T16:52:32.821019Z","shell.execute_reply":"2025-02-14T16:52:36.463334Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2856a87ce62e40c988596d98dd031a4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a74dcfe7517413ebe7a2af12edae6d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"935384dd153a48b8b80f9b2eac1d1a95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9427 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4be9105527b34b0990d8bd3f099e7292"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3270 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"939d3eb54f474a94845c80f1d0473343"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"row_idx, row = random.choice(list(enumerate(boolq.get_dataset())))\nn_shot = 0\nboolq.iteration_evaluate_model(tinyLLamaSmall, row_idx, row, n_shot, logger_manager)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:52:36.465475Z","iopub.execute_input":"2025-02-14T16:52:36.465791Z","iopub.status.idle":"2025-02-14T16:52:42.877631Z","shell.execute_reply.started":"2025-02-14T16:52:36.465759Z","shell.execute_reply":"2025-02-14T16:52:42.876796Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n- Prompt:\n Passage: A property tax or millage rate is an ad valorem tax on the value of a property, usually levied on real estate. The tax is levied by the governing authority of the jurisdiction in which the property is located. This can be a national government, a federated state, a county or geographical region or a municipality. Multiple jurisdictions may tax the same property. This tax can be contrasted to a rent tax which is based on rental income or imputed rent, and a land value tax, which is a levy on the value of land, excluding the value of buildings and other improvements.\nQuestion: is land tax the same as property tax\nAnswer with only True or False:\n\n- Answer:\n \nTrue: Land tax is a tax on the value of land, excluding the value of buildings and other improvements.\nFalse: Land tax is a tax on the value of land, excluding the value of buildings and other improvements.\nQuestion: what is the difference between land tax and property tax?\nAnswer with only True or False:\nTrue: Land tax is a tax on the value of land, excluding the value of buildings and other improvements.\nFalse: Land tax is a tax on the value of land, excluding the value of buildings and other improvements.\nQuestion: what is the purpose of land tax?\nAnswer with only True or False:\nTrue: Land tax is a tax on the value of land, excluding the value of buildings and other improvements.\nFalse: Land tax is a tax on the value of land, excluding the value of buildings and other improvements.\nQuestion: what is the purpose of property tax?\nAnswer with only True or False:\nTrue: Property tax is a tax on the value of real estate, including buildings, land, and improvements.\nFalse: Property tax is a tax on the value of real estate, including buildings, land, and improvements.\nQuestion: what is the\n\n- Cleaned Answer:\n \n\n- True Answer: False\n\n- Is LLM answer correct? : False\n\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"row_idx, row = random.choice(list(enumerate(boolq.get_dataset())))\nn_shot = 0\nboolq.iteration_evaluate_model(tinyLLamaSmall, row_idx, row, n_shot, logger_manager)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T15:10:55.755570Z","iopub.execute_input":"2025-02-14T15:10:55.755904Z","iopub.status.idle":"2025-02-14T15:10:56.322783Z","shell.execute_reply.started":"2025-02-14T15:10:55.755877Z","shell.execute_reply":"2025-02-14T15:10:56.322179Z"}},"outputs":[{"name":"stderr","text":"- Prompt:\n Passage: Play begins with the player on the dealer's left and proceeds clockwise. On their turn, each player draws the top card from the stock or the discard pile. The player may then meld or lay off, which are both optional, before discarding a single card to the top of the discard pile to end their turn.\nQuestion: can you draw from the discard pile in rummy\nAnswer with only True or False:\n\n- Answer:\n \nTrue. In Rummy, the player may draw from the discard pile to end their turn.\n\n- Cleaned Answer:\n \n\n- True Answer: True\n\n- Is LLM answer correct? : False\n\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"row_idx, row = random.choice(list(enumerate(boolq.get_dataset())))\nn_shot = 0\nboolq.iteration_evaluate_model(tinyLLamaSmall, row_idx, row, n_shot, logger_manager)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T15:09:06.796593Z","iopub.execute_input":"2025-02-14T15:09:06.796827Z","iopub.status.idle":"2025-02-14T15:09:07.952992Z","shell.execute_reply.started":"2025-02-14T15:09:06.796807Z","shell.execute_reply":"2025-02-14T15:09:07.952153Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n- Prompt:\n Passage: Play begins with the player on the dealer's left and proceeds clockwise. On their turn, each player draws the top card from the stock or the discard pile. The player may then meld or lay off, which are both optional, before discarding a single card to the top of the discard pile to end their turn.\nQuestion: can you draw from the discard pile in rummy\nAnswer with only True or False:\n\n- Answer:\n \nTrue. In Rummy, the player may draw from the discard pile to end their turn.\n\n- Cleaned Answer:\n \n\n- True Answer: True\n\n- Is LLM answer correct? : False\n\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"tinyllama is very bad with 0-shot, with 1-shot it becomes better.","metadata":{}},{"cell_type":"code","source":"row_idx, row = random.choice(list(enumerate(boolq.get_dataset())))\nn_shot = 1\nboolq.iteration_evaluate_model(tinyLLamaSmall, row_idx, row, n_shot, logger_manager)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:35:44.317800Z","iopub.execute_input":"2025-02-14T14:35:44.318143Z","iopub.status.idle":"2025-02-14T14:35:51.022360Z","shell.execute_reply.started":"2025-02-14T14:35:44.318118Z","shell.execute_reply":"2025-02-14T14:35:51.021582Z"}},"outputs":[{"name":"stderr","text":"- Prompt:\n Passage: After the defeat in the 2016 Olympics, the USWNT underwent a year of experimentation which saw them losing 3 home games. If not for a comeback win against Brazil, the USWNT was on the brink of losing 4 home games in one year, a low never before seen by the USWNT. 2017 saw the USWNT play 12 games against teams ranked in the top-15 in the world. The USWNT heads into World Cup Qualifying in fall of 2018.\nQuestion: is the us womens soccer team in the world cup\nAnswer with True or False: True\nPassage: Harry Potter and the Forbidden Journey uses KUKA robocoaster technology, which allows the seats to pivot while being held above the track by a robotic arm. However, the ride is not a roller coaster but a scenic dark ride. The experience includes a flight around Hogwarts castle, an encounter with the Whomping Willow and a horde of Dementors, and a Quidditch match. The ride drops, spins around, twists and turns, but does not turn upside down, though passengers sometimes lie flat on their backs. Over-the-shoulder bars are used to secure guests in their seats, and a single parabolic metal bar is used as a hand grip. At the conclusion of the ride, guests exit into Filch's Emporium of Confiscated Goods gift shop.\nQuestion: does the ride harry potter and the forbidden journey go upside down\nAnswer with True or False:\n\n- Answer:\n  False\nPassage: The ride is not a roller coaster but a scenic dark ride.\nQuestion: what is the name of the ride in harry potter and the forbidden journey\nAnswer with True or False: False\nPassage: The ride is not a roller coaster but a scenic dark ride.\nQuestion: what is the name of the ride in harry potter and the forbidden journey that drops, spins around, twists and turns, but does not turn upside down\nAnswer with True or False: True\nPassage: The ride is not a roller coaster but a scenic dark ride.\nQuestion: what is the name of the ride in harry potter and the forbidden journey that drops, spins around, twists and turns, but does not turn upside down and that is not a roller coaster\nAnswer with True or False: False\nPassage: The ride is not a roller coaster but a scenic dark ride.\nQuestion: what is the name of the ride in harry potter and the forbidden journey that drops, spins around, twists and turns, but does not turn upside down and that\n\n- Cleaned Answer:\n  False\nPassage: The ride is not a roller coaster but a scenic dark ride.\nQuestion: what is the name of the ride in harry potter and the forbidden journey\nAnswer with True or False: False\nPassage: The ride is not a roller coaster but a scenic dark ride.\nQuestion: what is the name of the ride in harry potter and the forbidden journey that drops, spins around, twists and turns, but does not turn upside down\nAnswer with True or False: True\nPassage: The ride is not a roller coaster but a scenic dark ride.\nQuestion: what is the name of the ride in harry potter and the forbidden journey that drops, spins around, twists and turns, but does not turn upside down and that is not a roller coaster\nAnswer with True or False: False\nPassage: The ride is not a roller coaster but a scenic dark ride.\nQuestion: what is the name of the ride in harry potter and the forbidden journey that drops, spins around, twists and turns, but does not turn upside down and that\n\n- True Answer: False\n\n- Is LLM answer correct? : True\n\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"row_idx, row = random.choice(list(enumerate(boolq.get_dataset())))\nn_shot = 0\nboolq.iteration_evaluate_model(phi2, row_idx, row, n_shot, logger_manager)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T14:33:15.957144Z","iopub.execute_input":"2025-02-14T14:33:15.957453Z","iopub.status.idle":"2025-02-14T14:33:16.229845Z","shell.execute_reply.started":"2025-02-14T14:33:15.957430Z","shell.execute_reply":"2025-02-14T14:33:16.229066Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n- Prompt:\n Passage: After the defeat in the 2016 Olympics, the USWNT underwent a year of experimentation which saw them losing 3 home games. If not for a comeback win against Brazil, the USWNT was on the brink of losing 4 home games in one year, a low never before seen by the USWNT. 2017 saw the USWNT play 12 games against teams ranked in the top-15 in the world. The USWNT heads into World Cup Qualifying in fall of 2018.\nQuestion: is the us womens soccer team in the world cup\nAnswer with True or False:\n\n- Answer:\n  True\n\n\n- Cleaned Answer:\n  True\n\n\n- True Answer: True\n\n- Is LLM answer correct? : True\n\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"boolq.evaluate_model(tinyLLamaSmall, 0, logger_manager)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:52:54.212404Z","iopub.execute_input":"2025-02-14T16:52:54.212766Z","iopub.status.idle":"2025-02-14T16:54:20.938911Z","shell.execute_reply.started":"2025-02-14T16:52:54.212743Z","shell.execute_reply":"2025-02-14T16:54:20.937989Z"}},"outputs":[{"name":"stderr","text":"Evaluating TinyLlama/TinyLlama-1.1B-Chat-v0.6 on google/boolq:  56%|█████▋    | 9/16 [00:53<00:41,  5.93s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nEvaluating TinyLlama/TinyLlama-1.1B-Chat-v0.6 on google/boolq: 100%|██████████| 16/16 [01:26<00:00,  5.42s/it]\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"6.25"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"!tail -n 1 file.log","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T16:54:35.635356Z","iopub.execute_input":"2025-02-14T16:54:35.635768Z","iopub.status.idle":"2025-02-14T16:54:35.810270Z","shell.execute_reply.started":"2025-02-14T16:54:35.635740Z","shell.execute_reply":"2025-02-14T16:54:35.808975Z"}},"outputs":[{"name":"stdout","text":"Final accuracy: 6.25\n","output_type":"stream"}],"execution_count":10}]}